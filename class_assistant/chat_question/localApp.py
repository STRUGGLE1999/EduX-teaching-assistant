import time
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer
import chromadb
from chromadb.config import Settings
from openai import OpenAI
import fitz

# -------------------------------
# å…¨å±€åˆå§‹åŒ–ï¼šç”¨äºè°ƒç”¨æ–‡æœ¬ç”Ÿæˆä¸åµŒå…¥æ¥å£
client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key="COEZ2XUAPTLNQQOL1LUMPARXYDDCXYUYJKOHAB83",
    default_headers={"X-Failover-Enabled": "true"}
)

start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

# -------------------------------
# PDF æ–‡ä»¶åŠ è½½ç±»
class PDFFileLoader():
    def __init__(self, file) -> None:
        self.paragraphs = self.extract_text_from_pdf(file)

    def getParagraphs(self):
        return self.paragraphs

    def extract_text_from_pdf(self, file):
        '''ä½¿ç”¨ PyMuPDF æå–æ–‡æœ¬'''
        paragraphs = []
        doc = fitz.open(stream=file.read(), filetype="pdf")  # ç›´æ¥ä»ä¸Šä¼ çš„ file å¯¹è±¡è¯»å–
        for page in doc:
            lines = [l.strip() for l in page.get_text("text").split('\n') if l.strip()]
            # æ¯ 5 è¡Œåˆå¹¶
            for i in range(0, len(lines), 5):
                para = ' '.join(lines[i:i+5])
                if len(para) > 50:
                    paragraphs.append(para)
        return paragraphs


# -------------------------------
# æ»‘åŠ¨çª—å£åµŒå…¥ï¼ˆå¤„ç†é•¿æ–‡æœ¬ï¼‰
def sliding_window_embedding(text, window_size=512, step_size=256):
    """
    å¯¹é•¿æ–‡æœ¬è¿›è¡Œæ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œæ¯ä¸ªçª—å£ç”Ÿæˆä¸€ä¸ªåµŒå…¥å‘é‡ã€‚
    window_sizeï¼šæ¯ä¸ªçª—å£çš„å¤§å°ï¼ˆè¯æ•°ï¼‰
    step_sizeï¼šçª—å£ç§»åŠ¨æ­¥é•¿ï¼ˆè¯æ•°ï¼‰
    """
    words = text.split()
    embeddings = []
    for start_idx in range(0, len(words), step_size):
        end_idx = min(start_idx + window_size, len(words))
        window = ' '.join(words[start_idx:end_idx])
        embedding = get_embeddings([window])[0]  # è·å–åµŒå…¥å‘é‡
        embeddings.append(embedding)
    return embeddings


# -------------------------------
# ä¿®æ”¹åçš„åµŒå…¥éƒ¨åˆ†ï¼šä½¿ç”¨è¿œç¨‹æ¥å£è·å–æ–‡æœ¬åµŒå…¥
def get_embeddings(texts, batch_size=20):
    """
    åˆ†æ‰¹è°ƒç”¨è¿œç¨‹åµŒå…¥æ¥å£ï¼Œé¿å…è¶…è¿‡å•æ¬¡æœ€å¤§æ”¯æŒæ•°é‡ï¼ˆå¦‚25æ¡ï¼‰ã€‚
    """
    all_embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        try:
            response = client.embeddings.create(
                model="bge-m3",  # ä½ ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
                input=batch
            )
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)
        except Exception as e:
            print(f"æ‰¹æ¬¡ {i//batch_size + 1} åµŒå…¥å‡ºé”™: {e}")
            all_embeddings.extend([None] * len(batch))  # æˆ–è€…æŠ›å¼‚å¸¸
    return all_embeddings


# -------------------------------
# ChromaDB è¿æ¥ç±»
class MyVectorDBConnector:
    def __init__(self, collection_name, embedding_fn):
        chroma_client = chromadb.Client(Settings(allow_reset=True))
        # åˆ›å»ºæˆ–è·å– collectionï¼Œå¹¶ä½¿ç”¨ HNSW ç´¢å¼•
        self.collection = chroma_client.get_or_create_collection(name=collection_name)
        self.embedding_fn = embedding_fn

    def add_documents(self, documents):
        embeddings = self.embedding_fn(documents)  # ä½¿ç”¨åµŒå…¥å‡½æ•°
        self.collection.add(
            documents=documents,
            embeddings=embeddings,  # ä¼ é€’æ–‡æ¡£çš„åµŒå…¥
            ids=[f"id{i}" for i in range(len(documents))]
        )

    def search(self, query, top_n=5, similarity_threshold=None, mode="auto"):
        """
        mode å‚æ•°:
            - "auto": é»˜è®¤æ¨¡å¼ï¼Œè‡ªåŠ¨è®¾å®šé˜ˆå€¼å¹¶è¿‡æ»¤
            - "threshold": ä½¿ç”¨æŒ‡å®šé˜ˆå€¼è¿‡æ»¤
            - "topn": ç›´æ¥å–è·ç¦»æœ€å°çš„å‰ top_n æ¡
        """
        # è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
        query_embedding = self.embedding_fn([query])[0]
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_n * 2  # å¤šæ‹¿ä¸€äº›å›æ¥ï¼Œä¾¿äºåå¤„ç†
        )

        all_docs = [doc for doc_list in results['documents'] for doc in doc_list]
        all_distances = [score for score_list in results['distances'] for score in score_list]
        min_distance = min(all_distances)
        max_distance = max(all_distances)
        avg_distance = sum(all_distances) / len(all_distances)

        print(f"ğŸ” raw docs: {results['documents']}")
        print(f"ğŸ” raw distances: {results['distances']}")
        print(f"ğŸ“ˆ è·ç¦»ç»Ÿè®¡ - æœ€å°å€¼: {min_distance:.2f}, æœ€å¤§å€¼: {max_distance:.2f}, å¹³å‡å€¼: {avg_distance:.2f}")

        filtered_results = []

        if mode == "auto":
            # è‡ªåŠ¨æ¨¡å¼ï¼šåŸºäºæœ€å°å€¼åŠ¨æ€è®¾é˜ˆå€¼
            if similarity_threshold is None:
                similarity_threshold = min_distance + 50  # å®¹å·®è®¾å®š
                print(f"âš™ï¸ è‡ªåŠ¨è®¾å®š similarity_threshold = {similarity_threshold:.2f}")
            for doc, score in zip(all_docs, all_distances):
                if score <= similarity_threshold:
                    filtered_results.append((doc, score))

        elif mode == "threshold":
            # æ‰‹åŠ¨ä¼ å…¥é˜ˆå€¼
            if similarity_threshold is None:
                raise ValueError("åœ¨ mode='threshold' æ¨¡å¼ä¸‹ï¼Œè¯·æ‰‹åŠ¨ä¼ å…¥ similarity_threshold")
            for doc, score in zip(all_docs, all_distances):
                if score <= similarity_threshold:
                    filtered_results.append((doc, score))

        elif mode == "topn":
            # ç›´æ¥å–è·ç¦»æœ€è¿‘çš„ top_n ä¸ª
            combined = list(zip(all_docs, all_distances))
            combined.sort(key=lambda x: x[1])  # æŒ‰è·ç¦»å‡åº
            filtered_results = combined[:top_n]

        else:
            raise ValueError(f"æœªçŸ¥çš„ mode: {mode}")

        final_docs = [doc for doc, score in filtered_results]
        print(f"âœ… é€‰å‡ºçš„æ–‡æ¡£æ•°: {len(final_docs)}")
        return {'documents': final_docs}



# -------------------------------
# Prompt æ‹¼æ¥å‡½æ•°
def build_prompt(prompt_template, **kwargs):
    '''å°† Prompt æ¨¡æ¿èµ‹å€¼'''
    prompt = prompt_template
    for k, v in kwargs.items():
        if isinstance(v, str):
            val = v
        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):
            val = '\n'.join(v)
        else:
            val = str(v)
        prompt = prompt.replace(f"__{k.upper()}__", val)
    return prompt


# -------------------------------
# Prompt æ¨¡æ¿å®šä¹‰
prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä¸­æ–‡æ™ºèƒ½åŠ©æ•™ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯ã€ç»“åˆè‡ªå·±çš„çŸ¥è¯†ç”¨ä¸­æ–‡æ€»ç»“å‡ºé—®é¢˜çš„ç­”æ¡ˆï¼Œå‡†ç¡®åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
ç¡®ä¿ä½ çš„å›å¤å®Œå…¨ä¾æ®ä¸‹è¿°å·²çŸ¥ä¿¡æ¯ï¼Œå›ç­”çš„å†…å®¹åŠ¡å¿…ç¬¦åˆä¸­æ–‡è¯­æ³•è§„èŒƒï¼Œé€šé¡ºæµç•…ï¼Œç®€çŸ­ç²¾ç‚¼å¹¶ä¸”å‡†ç¡®æ— è¯¯ã€‚
å¦‚æœä¸‹è¿°å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤"ä¸æ•™ææ— å…³ï¼Œæˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜"ã€‚

å·²çŸ¥ä¿¡æ¯:
__INFO__

ç”¨æˆ·é—®ï¼š
__QUERY__

è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
"""

prompt_question = """
ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä¸­æ–‡å‡ºé¢˜åŠ©æ‰‹,ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯ä¸ºç”¨æˆ·å‡ºç»™å®šé¢˜ç›®æ•°é‡çš„é¢˜.
æ¯”å¦‚é¢˜ç›®ç±»å‹æ˜¯é€‰æ‹©é¢˜ï¼Œé¢˜ç›®æ•°é‡æ˜¯3ï¼Œé‚£ä¹ˆä½ å°±å¸®ç”¨æˆ·å‡º3é“é€‰æ‹©é¢˜ï¼Œå¹¶ä¸”æ¯é“é¢˜çš„éƒ½è¦ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä»¥æ­¤ç±»æ¨ã€‚
ç¡®ä¿å‡ºçš„é¢˜ç›®å¿…é¡»ä¸ç”¨æˆ·è¦æ±‚ã€é¢˜ç›®ç±»å‹ã€é¢˜ç›®æ•°é‡çš„ç›¸ç¬¦ã€‚ä¸è¦èƒ¡ç¼–ä¹±é€ ï¼Œä¸è¦æœ‰æ— å…³çš„ç¬¦å·ã€‚
å¦‚æœé¢˜ç›®ç±»å‹æ˜¯åˆ¤æ–­é¢˜ï¼Œé‚£ä¹ˆä½ è¦å‡ºéœ€è¦åˆ¤æ–­å¯¹é”™çš„é¢˜ï¼Œå¹¶ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä¸è¦è¯´å¤šä½™çš„è¯ã€‚
å¦‚æœé¢˜ç›®ç±»å‹æ˜¯é—®ç­”é¢˜ï¼Œé‚£ä¹ˆä½ éœ€è¦ç”¨ä¸­æ–‡å‡ºé¢˜ï¼Œå¹¶ä¸”ç”¨ä¸­æ–‡ç»™å‡ºæ­£ç¡®çš„å›ç­”ï¼Œå°½é‡ç®€çŸ­ç²¾ç‚¼ã€‚
å¦‚æœä¸‹è¿°å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å‡ºé¢˜ï¼Œè¯·ç›´æ¥å›å¤"ä¸æ•™ææ— å…³ï¼Œæˆ‘æ— æ³•å®Œæˆæ‚¨çš„ä»»åŠ¡"ã€‚
å¦‚æœèƒ½å›ç­”ï¼Œé‚£ä¹ˆprompté‡Œçš„è¯è¯·ä¸è¦è¾“å‡º

å·²çŸ¥ä¿¡æ¯:
__INFO__

ç”¨æˆ·è¦æ±‚ï¼š
__QUERY__

é¢˜ç›®ç±»å‹ï¼š
__TYPE__

é¢˜ç›®æ•°é‡ï¼š
__NUM__

è¯·ç”¨ä¸­æ–‡è¡¨è¾¾ã€‚
"""

prompt_code = """
ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä»£ç æ™ºèƒ½åŠ©æ‰‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ‰®æ¼”ä¸‹è¿°ç»™å®šçš„è§’è‰²å›ç­”ç”¨æˆ·ä¸ä»£ç æœ‰å…³çš„é—®é¢˜ã€‚
å¦‚æœä¸‹è¿°å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤"æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜"ã€‚

è§’è‰²:
__ROLE__

ç”¨æˆ·é—®ï¼š
__QUERY__

è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¹¶ä¸”ä¸è¦è¾“å‡ºprompté‡Œçš„å†…å®¹ã€‚
"""

# -------------------------------
# ä¿®æ”¹åçš„ get_completion å‡½æ•°ï¼Œè°ƒç”¨æ˜‡è…¾èµ„æºåŒ…å†…çš„æ–‡æœ¬ç”Ÿæˆæ¥å£
def get_completion(prompt):
    """è°ƒç”¨æ˜‡è…¾å¹³å°èµ„æºåŒ…å†…çš„ QwQ-32B æ¨¡å‹æ¥å£ç”Ÿæˆæ–‡æœ¬å“åº”ã€‚"""
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªå–„äºç›´æ¥ç®€æ´å›ç­”çš„ä¸­æ–‡åŠ©æ•™ã€‚è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€‚"
        },
        {
            "role": "user",
            "content": prompt
        }
    ]
    response = client.chat.completions.create(
        model="internlm3-8b-instruct",        # å¦‚éœ€æ›´æ¢ä¸ºå…¶ä»–æ¨¡å‹ï¼Œå¯é€‰æ‹© DeepSeek-R1-Distill-Qwen-32Bã€InternLM3-8B-Instruct ç­‰
        stream=False,          # éæµå¼è°ƒç”¨ï¼Œè¿”å›å®Œæ•´å“åº”
        max_tokens=1024,
        temperature=1,
        top_p=0.2,
        extra_body={"top_k": 50},
        frequency_penalty=0,
        messages=messages,
    )
    # å‡è®¾æ¥å£è¿”å›æ ¼å¼ä¸ºï¼š{"choices": [{"message": {"content": "ç”Ÿæˆçš„å›ç­”å†…å®¹"}}]}
    return response.choices[0].message.content


# -------------------------------
# Chat_Bot ç±»ï¼Œä¿æŒåŸæœ‰ç»“æ„ï¼Œä»… llm_api åŠ get_embeddings è°ƒç”¨æ–¹å¼æ”¹å˜
class Chat_Bot:
    def __init__(self, n_results=5, similarity_threshold=3):
        self.llm_api = get_completion
        self.n_results = n_results
        self.similarity_threshold = similarity_threshold
        self.vector_db = None  # æå‰è®¾ç½®å±æ€§ä¸º Noneï¼Œé¿å…æœªå®šä¹‰é”™è¯¯

    def createVectorDB(self, file):
        print(file)
        pdf_loader = PDFFileLoader(file)
        # ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•æ¥å¤„ç†é•¿æ–‡æœ¬
        paragraphs = pdf_loader.getParagraphs()
        # åˆ›å»ºå‘é‡æ•°æ®åº“å¯¹è±¡
        self.vector_db = MyVectorDBConnector("demo", get_embeddings)
        # æ‰¹é‡æ·»åŠ æ–‡æ¡£
        self.vector_db.add_documents(paragraphs)

    def chat(self, user_query):
        if self.vector_db is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ createVectorDB(file) æ–¹æ³•åŠ è½½æ•™æå¹¶æ„å»ºå‘é‡æ•°æ®åº“ã€‚")
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œå¹¶åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
        search_results = self.vector_db.search(user_query, self.n_results, mode="auto")
        print('----------', search_results)
        # 2. æ„å»º Prompt
        flattened_docs = [''.join(doc) if isinstance(doc, list) else str(doc) for doc in search_results['documents']]
        prompt = build_prompt(prompt_template, info='\n'.join(flattened_docs), query=user_query)
        # 3. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
        response = self.llm_api(prompt)
        return response

    def question(self,user_query,type,num):
        search_results = self.vector_db.search(user_query,self.n_results,mode="auto")
        flattened_docs = [''.join(doc) if isinstance(doc, list) else str(doc) for doc in search_results['documents']]
        prompt = build_prompt(prompt_question, info='\n'.join(flattened_docs), query=user_query, type=type, num=num)
        response = self.llm_api(prompt)
        return response

    def chat_without_file(self, user_query):
        prompt = build_prompt(prompt_template, info="æ— ç›¸å…³æ•™æä¿¡æ¯ï¼Œç”¨æˆ·é—®é¢˜æ˜¯é€šç”¨æ€§é—®é¢˜", query=user_query)
        response = self.llm_api(prompt)
        return response